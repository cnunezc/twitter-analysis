{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= \n",
    "consumer_secret= \n",
    "access_token= \n",
    "access_token_secret= \n",
    "bearer_token="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "client = tw.Client(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "pd.set_option('display.max_rows', 10000000)\n",
    "pd.set_option('display.max_columns', 1000000)\n",
    "\n",
    "\n",
    "\n",
    "def get_related_tweets(key_word):\n",
    "\n",
    "    twitter_users = []\n",
    "    tweet_time = []\n",
    "    tweet_string = [] \n",
    "    for tweet in tweepy.Cursor(api.search_tweets,q=key_word, count=1000).items(1000):\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "                if tweet.lang == \"en\":\n",
    "                    twitter_users.append(tweet.user.name)\n",
    "                    tweet_time.append(tweet.created_at)\n",
    "                    tweet_string.append(tweet.text)\n",
    "                    #print([tweet.user.name,tweet.created_at,tweet.text])\n",
    "    df = pd.DataFrame({'name':twitter_users, 'time': tweet_time, 'tweet': tweet_string})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_related_tweets(\"Superbowl Halftime Show\")\n",
    "df.to_csv('sss.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"VMA.csv\",index_col=0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text_1(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special text in brackets ([chorus],[guitar],etc)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    # Remove quotes\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    # Remove new line \\n \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    # Remove stop_word\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_df.tweet.apply(clean_text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print(\"%-20s %-20s %-20s\"% (\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in verb_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))\n",
    "print(\"--\")\n",
    "for word in noun_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))\n",
    "print(\"--\")\n",
    "for word in adjec_list:\n",
    "    print(\"%-20s %-20s %-20s\"%(word, porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "txt = \"Remember when you were young, you shone like the sun Shine on you crazy diamond\"\n",
    "pos_tag(word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tag(text):\n",
    "    lemma=[]\n",
    "    for i,j in pos_tag(word_tokenize(text)) :\n",
    "        p=j[0].lower()\n",
    "        if p in ['j','n','v']:\n",
    "            if p == 'j':\n",
    "                p = 'a'\n",
    "            lemma.append(wnl.lemmatize(i,p))\n",
    "        else :\n",
    "            lemma.append(wnl.lemmatize(i))    \n",
    "    return ' '.join(lemma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(data_clean.tweet.apply(lemmatize_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_csv('vma_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_clean = pd.read_csv(\"vma_clean.csv\",index_col=0)\n",
    "data_clean = data_clean.reset_index()\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.tweet)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(data_clean.tweet)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\n",
    "data_tfidf.index = data_clean.index\n",
    "data_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 30 words \n",
    "\n",
    "data=data_dtm.transpose()\n",
    "\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the most common top words \n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each artist\n",
    "words = []\n",
    "for artist in data.columns:\n",
    "    top = [word for (word, count) in top_dict[artist]]\n",
    "    for t in top:\n",
    "        words.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If more than half of the comedians have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 8]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "stop_words\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.tweet)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words2=[]\n",
    "for w in stop_words:\n",
    "    stop_words2.append(w)\n",
    "\n",
    "#stop_words2.extend(['la','li','ooh','bird','number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(collocations=False,stopwords=stop_words2, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = ['vma','dont','freedman','mtv vma', 'video', 'mtv','vma performance', 'fba', 'look', 'video', 'dont', 'think', 'britney', 'grammy', 'know', 'still', 'perform', 'need', 'one', 'go', 'vmas', 'day', 'even', 'year', 'say', 'time', 'pokemongoapp']\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(tag))\n",
    "pat\n",
    "data_clean['tweet'] = data_clean['tweet'].str.replace(pat, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text2 = data_clean\n",
    "wordcloud2 = WordCloud(background_color=\"white\").generate(' '.join(text2['tweet']))\n",
    "# Generate plot\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
